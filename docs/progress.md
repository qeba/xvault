# xVault Development Progress

**Last Updated:** 2025-12-29

This file tracks the implementation progress of xVault features based on the development sequence in [docs/dev-start.md](dev-start.md) and milestones in [docs/plan.md](plan.md).

## Technology Stack

| Component | Technology | Status |
|-----------|------------|--------|
| **Hub API** | Go + Fiber v2 | ‚úÖ Configured |
| **Worker** | Go | ‚úÖ Configured |
| **Frontend** | Vue.js | üîÑ Deferred to v2+ |
| **Database** | PostgreSQL | ‚úÖ Docker configured |
| **Queue** | Redis | ‚úÖ Docker configured |
| **Storage v0** | Worker-local filesystem | ‚úÖ Docker volume configured |
| **Storage v1** | S3/Garage | üîÑ Deferred |

---

## Legend

- ‚è≥ **Not Started** - Task not yet begun
- üöß **In Progress** - Currently being worked on
- ‚úÖ **Done** - Completed and tested
- ‚ö†Ô∏è **Blocked** - Waiting on dependencies or decisions
- üîÑ **Deferred** - Moved to later phase

---

## Foundation (Phase 0)

**Status**: ‚úÖ **Complete** - Scaffolding done, Docker setup ready

| Step | Task | Status | Notes |
|------|------|--------|-------|
| 0.1 | Monorepo structure (`/cmd`, `/internal`, `/pkg`, `/migrations`, `/deploy`) | ‚úÖ | Repo scaffolded |
| 0.2 | `go.mod` with module `xvault` and Go 1.25 | ‚úÖ | |
| 0.3 | Hub Dockerfile ([deploy/docker/hub/Dockerfile](deploy/docker/hub/Dockerfile)) | ‚úÖ | Multi-stage, distroless base |
| 0.4 | Worker Dockerfile ([deploy/docker/worker/Dockerfile](deploy/docker/worker/Dockerfile)) | ‚úÖ | Multi-stage, distroless base |
| 0.5 | Docker Compose with Postgres, Redis, Hub, Worker | ‚úÖ | |
| 0.6 | Environment variables (.env.example) | ‚úÖ | |
| 0.7 | Placeholder Hub service (health check, basic routes) | ‚úÖ | Fiber v2, listens on :8080 |
| 0.8 | Placeholder Worker service (Redis ping, storage base config) | ‚úÖ | Connects to Redis, awaits jobs |

**Storage Path Note**: Worker storage base is `/var/lib/xvault/backups` (mounted volume in Compose)

---

## Step 4: Database Migrations

**Status**: ‚úÖ **Complete** - Full schema implemented with Goose migration tool

**Goal**: Implement the minimal v0 schema from [docs/data-model.md](data-model.md)

**Deliverables**:
- `/migrations` directory with SQL migration files
- Migration runner in Hub (startup or `migrate` command)

| Task | Status | Notes |
|------|--------|-------|
| 4.1 | Set up migration tool/library | ‚úÖ | Using Goose v3 (github.com/pressly/goose/v3) |
| 4.2 | Create `tenants` table | ‚úÖ | `id`, `name`, `plan`, timestamps |
| 4.3 | Create `users` table | ‚úÖ | `id`, `tenant_id`, `email`, `password_hash`, `role`, timestamps |
| 4.4 | Create `credentials` table | ‚úÖ | `id`, `tenant_id`, `kind`, `ciphertext`, `key_id`, timestamps |
| 4.5 | Create `tenant_keys` table | ‚úÖ | `id`, `tenant_id`, `algorithm`, `public_key`, `encrypted_private_key`, `key_status`, timestamps |
| 4.6 | Create `sources` table | ‚úÖ | `id`, `tenant_id`, `type`, `name`, `status`, `config` (JSONB), `credential_id`, timestamps |
| 4.7 | Create `schedules` table | ‚úÖ | `id`, `tenant_id`, `source_id`, `cron`/`interval_minutes`, `timezone`, `enabled`, `retention_policy` (JSONB), timestamps |
| 4.8 | Create `workers` table | ‚úÖ | `id`, `name`, `status`, `capabilities` (JSONB), `storage_base_path`, `last_seen_at`, timestamps |
| 4.9 | Create `jobs` table | ‚úÖ | `id`, `tenant_id`, `source_id`, `type`, `status`, `priority`, `target_worker_id`, `lease_expires_at`, `attempt`, `payload` (JSONB), timestamps, error fields |
| 4.10 | Create `snapshots` table | ‚úÖ | `id`, `tenant_id`, `source_id`, `job_id`, `status`, `size_bytes`, duration fields, `manifest_json`, encryption metadata, locator fields (`storage_backend`, `worker_id`, `local_path`), timestamps |
| 4.11 | Create `audit_events` table (optional for v0) | ‚úÖ | Included for complete audit trail |
| 4.12 | Add indexes/constraints per data-model.md | ‚úÖ | All indexes and foreign keys added |
| 4.13 | Hub runs migrations on startup OR provides `migrate` command | ‚úÖ | Supports `-migrate` flag and `HUB_AUTO_MIGRATE` env var |

**Implementation Details**:
- Migration file: [internal/hub/database/migrations/0001_init.sql](internal/hub/database/migrations/0001_init.sql)
- Database package: [internal/hub/database/migrate.go](internal/hub/database/migrate.go)
- Hub CLI: `./bin/hub -migrate` (run migrations and exit)
- Hub CLI: `./bin/hub -migrate-status` (show migration status)
- Auto-migrate: Set `HUB_AUTO_MIGRATE=true` to run migrations on startup

**Database Types (Enums)**:
- `user_role`: owner, admin, member
- `key_status`: active, rotated, disabled
- `credential_kind`: source, storage
- `source_type`: ssh, sftp, ftp, mysql, postgres, wordpress
- `source_status`: active, disabled
- `schedule_status`: enabled, disabled
- `worker_status`: online, offline, draining
- `job_type`: backup, restore, delete_snapshot
- `job_status`: queued, running, finalizing, completed, failed, canceled
- `snapshot_status`: completed, failed
- `storage_backend`: local_fs, s3

---

## Step 5: First Runnable Slice (End-to-End)

**Status**: ‚úÖ **Complete** - Full end-to-end backup pipeline tested and working

**Goal**: Prove end-to-end orchestration with smallest surface area

**Acceptance**:
- ‚úÖ One backup run results in a `snapshots` row with `storage_backend=local_fs`
- ‚úÖ A file exists on worker storage under `/var/lib/xvault/backups/tenants/{tenant_id}/sources/{source_id}/snapshots/{snapshot_id}/`
- ‚úÖ Worker successfully claims and processes backup jobs from Hub

**Connector Scope**: SSH/SFTP only initially (simplest, covers most use cases)

### 5.1 Hub: Tenant Management

| Task | Status | Notes |
|------|--------|-------|
| 5.1.1 | `POST /api/v1/tenants` endpoint | ‚úÖ | [`internal/hub/handlers/handlers.go:47`](internal/hub/handlers/handlers.go) |
| 5.1.2 | Generate tenant keypair on creation (Age/x25519) | ‚úÖ | [`internal/hub/service/service.go:34`](internal/hub/service/service.go) |
| 5.1.3 | Store tenant private key encrypted at rest | ‚úÖ | Using `HUB_ENCRYPTION_KEK` envelope encryption |
| 5.1.4 | `GET /api/v1/tenants/:id` endpoint | üîÑ | Not implemented yet (low priority) |

### 5.2 Hub: Source & Credential Management

| Task | Status | Notes |
|------|--------|-------|
| 5.2.1 | `POST /api/v1/credentials` endpoint | ‚úÖ | [`internal/hub/handlers/handlers.go:78`](internal/hub/handlers/handlers.go) |
| 5.2.2 | Envelope encryption implementation | ‚úÖ | [`pkg/crypto/age.go:72`](pkg/crypto/age.go) |
| 5.2.3 | `POST /api/v1/sources` endpoint | ‚úÖ | [`internal/hub/handlers/handlers.go:103`](internal/hub/handlers/handlers.go) |
| 5.2.4 | `GET /api/v1/sources` list endpoint | ‚úÖ | [`internal/hub/handlers/handlers.go:126`](internal/hub/handlers/handlers.go) |
| 5.2.5 | Source config validation (SSH/SFTP) | üîÑ | Client-side validation only for v0 |

### 5.3 Hub: Job Queue & Orchestration

| Task | Status | Notes |
|------|--------|-------|
| 5.3.1 | `POST /api/v1/jobs` endpoint (manual trigger) | ‚úÖ | [`internal/hub/handlers/handlers.go:166`](internal/hub/handlers/handlers.go) |
| 5.3.2 | Job payload format definition | ‚úÖ | [`pkg/types/types.go:58`](pkg/types/types.go) |
| 5.3.3 | Enqueue job to Redis | ‚úÖ | Uses `xvault:jobs:queue` key |
| 5.3.4 | Internal: `POST /internal/jobs/claim` endpoint | ‚úÖ | Worker claims next queued job |
| 5.3.5 | Internal: `POST /internal/jobs/:id/complete` endpoint | ‚úÖ | Worker reports completion + snapshot |
| 5.3.6 | Internal: `GET /internal/credentials/:id` endpoint | ‚úÖ | Worker fetches encrypted creds |
| 5.3.7 | Internal: `GET /internal/tenants/:id/public-key` endpoint | ‚úÖ | Worker fetches tenant public key |
| 5.3.8 | Internal: `POST /internal/workers/register` endpoint | ‚úÖ | Worker registration |
| 5.3.9 | Internal: `POST /internal/workers/heartbeat` endpoint | ‚úÖ | Worker heartbeats |

### 5.4 Worker: Job Loop

| Task | Status | Notes |
|------|--------|-------|
| 5.4.1 | Redis job dequeue (blocking or polling) | ‚úÖ | Polling via Hub API claim endpoint |
| 5.4.2 | Claim job via Hub API | ‚úÖ | [`internal/worker/client/client.go:48`](internal/worker/client/client.go) |
| 5.4.3 | Fetch and decrypt credentials | ‚úÖ | JIT credential fetch via Hub API |
| 5.4.4 | Job lease management (heartbeat/renewal) | ‚úÖ | 30s heartbeat interval |
| 5.4.5 | Error handling and retry logic | ‚úÖ | Errors logged, next job claimed on failure |
| 5.4.6 | Graceful shutdown (finish current job) | ‚úÖ | SIGINT/SIGTERM handling with context cancel |

### 5.5 Worker: SSH/SFTP Connector

| Task | Status | Notes |
|------|--------|-------|
| 5.5.1 | SSH client connection | ‚úÖ | [`internal/worker/connector/sftp.go:37`](internal/worker/connector/sftp.go) |
| 5.5.2 | SFTP file download to temp dir | ‚úÖ | Uses `/tmp/gobackup/{job_id}/source-mirror/` |
| 5.5.3 | Recursive directory pull | ‚úÖ | SFTP walker with recursive pull |
| 5.5.4 | Error handling for connection failures | ‚úÖ | Errors propagated up for job failure reporting |

### 5.6 Worker: Packaging & Encryption

| Task | Status | Notes |
|------|--------|-------|
| 5.6.1 | Create tar archive from staged data | ‚úÖ | Simple ustar format implementation |
| 5.6.2 | Compress with Zstandard (zstd) | ‚úÖ | Using klauspost/compress/zstd |
| 5.6.3 | Encrypt with Age (tenant public key) | ‚úÖ | Using pkg/crypto age encryption |
| 5.6.4 | Generate `backup.tar.zst.enc` artifact | ‚úÖ | Full artifact pipeline |
| 5.6.5 | Generate `manifest.json` | ‚úÖ | IDs, sizes, hashes, encryption metadata |
| 5.6.6 | Generate `meta.json` | ‚úÖ | tenant_id, source_id, snapshot_id, job_id, worker_id |
| 5.6.7 | Cleanup temp directory | ‚úÖ | Aggressive cleanup after job |

### 5.7 Worker: Local Storage (v0)

| Task | Status | Notes |
|------|--------|-------|
| 5.7.1 | Create multi-tenant directory structure | ‚úÖ | Path: `/var/lib/xvault/backups/tenants/{tenant_id}/sources/{source_id}/snapshots/{snapshot_id}/` |
| 5.7.2 | Write artifact to durable path | ‚úÖ | [`internal/worker/storage/storage.go:36`](internal/worker/storage/storage.go) |
| 5.7.3 | Write manifest.json and meta.json | ‚úÖ | Both files written with artifact |

### 5.8 Hub: Snapshot Metadata

| Task | Status | Notes |
|------|--------|-------|
| 5.8.1 | Store snapshot record in database | ‚úÖ | [`internal/hub/repository/repository.go:432`](internal/hub/repository/repository.go) |
| 5.8.2 | Store snapshot locator | ‚úÖ | storage_backend, worker_id, local_path |
| 5.8.3 | `GET /api/v1/snapshots` list endpoint | ‚úÖ | [`internal/hub/handlers/handlers.go:345`](internal/hub/handlers/handlers.go) |
| 5.8.4 | `GET /api/v1/snapshots/:id` details endpoint | ‚úÖ | [`internal/hub/handlers/handlers.go:368`](internal/hub/handlers/handlers.go) |

### 5.9 End-to-End Integration Test

| Task | Status | Notes |
|------|--------|-------|
| 5.9.1 | Create tenant ‚Üí verify keypair generated | ‚úÖ | Tenant created with Age/x25519 keypair |
| 5.9.2 | Create source ‚Üí verify credentials encrypted | ‚úÖ | Credentials encrypted with platform KEK |
| 5.9.3 | Enqueue backup job ‚Üí verify appears in Redis | ‚úÖ | Job enqueued to Redis, status=queued |
| 5.9.4 | Worker claims job ‚Üí verify status=running | ‚úÖ | Worker claimed job via Hub API |
| 5.9.5 | Worker completes SSH/SFTP backup | ‚úÖ | **Real SSH server test: 10.0.100.85:/home/web/test** |
| 5.9.6 | Verify snapshot stored in worker filesystem | ‚úÖ | Artifact, manifest, meta.json all present |
| 5.9.7 | Verify snapshot record in Hub DB | ‚úÖ | Snapshot record with correct locator |
| 5.9.8 | List snapshots via API | ‚úÖ | API endpoint working |

**‚úÖ END-TO-END TEST COMPLETE**

Successfully backed up files from real SSH server `10.0.100.85:/home/web/test`:
- **Job ID**: `4e4dd30a-3493-4021-bc56-c5b5acf9aa06`
- **Snapshot ID**: `17a1cbfe36bbf9e35cea08da736b608f`
- **Files Pulled**: 2 files (100MB.bin + file = 104,857,600 bytes)
- **Artifact Size**: 104,886,398 bytes (encrypted + compressed)
- **Duration**: 593ms
- **Encryption**: age-x25519
- **Storage Path**: `/var/lib/xvault/backups/tenants/{tenant_id}/sources/{source_id}/snapshots/{snapshot_id}/`

**Files Created**:
```
/var/lib/xvault/backups/tenants/.../snapshots/17a1cbfe36bbf9e35cea08da736b608f/
‚îú‚îÄ‚îÄ backup.tar.zst.enc (104,886,398 bytes)
‚îú‚îÄ‚îÄ manifest.json (698 bytes)
‚îî‚îÄ‚îÄ meta.json (165 bytes)
```

**V0 Credential Encryption Note**: For v0, credentials are encrypted with the platform KEK (not tenant public key) so workers can decrypt them. This is a temporary approach for the MVP; v1 will use proper envelope encryption where workers can't decrypt credentials directly.

**Worker Dockerfile**: Changed from distroless to debian:bookworm-slim with dedicated `worker` user (UID 1000) to fix storage permission issues.

**Test Commands**:
```bash
# Create tenant
curl -X POST http://localhost:8080/api/v1/tenants \
  -H "Content-Type: application/json" -d '{"name":"my-tenant"}'

# Create credential (password base64 encoded)
curl -X POST http://localhost:8080/api/v1/credentials \
  -H "Content-Type: application/json" \
  -d '{"tenant_id":"...","kind":"source","plaintext":"dGVzdC1wYXNz"}'

# Create SSH source
curl -X POST http://localhost:8080/api/v1/sources \
  -H "Content-Type: application/json" \
  -d '{"tenant_id":"...","type":"ssh","name":"server","credential_id":"...","config":{"host":"10.0.100.85","port":22,"username":"web","paths":["/home/web/test"],"use_password":true}}'

# Enqueue job
curl -X POST "http://localhost:8080/api/v1/jobs?tenant_id=..." \
  -H "Content-Type: application/json" -d '{"source_id":"..."}'
```

**API Documentation**: See [docs/api-reference.md](api-reference.md) for complete API reference.

---

## Step 6: Retention & Cleanup (v0)

**Status**: ‚úÖ **Complete** - Full retention policy evaluation and cleanup pipeline implemented

**Goal**: Prevent unbounded disk growth as backups accumulate

| Task | Status | Notes |
|------|--------|-------|
| 6.1 | Retention policy evaluation in Hub | ‚úÖ | Parse `retention_policy` JSONB from schedules |
| 6.2 | Identify snapshots to delete per policy | ‚úÖ | Multiple policy types supported |
| 6.3 | Enqueue `delete_snapshot` jobs | ‚úÖ | Must target `snapshot.worker_id` |
| 6.4 | Worker: handle `delete_snapshot` job type | ‚úÖ | |
| 6.5 | Worker deletes local filesystem path | ‚úÖ | |
| 6.6 | Worker reports completion to Hub | ‚úÖ | |
| 6.7 | Hub updates snapshot status or deletes record | ‚úÖ | |

### Implementation Details

**Retention Policy Types** ([`pkg/types/types.go:230`](pkg/types/types.go:230)):
- `keep_last_n`: Keep the N most recent snapshots
- `keep_daily`: Keep one snapshot per day for N days
- `keep_weekly`: Keep one snapshot per week for N weeks
- `keep_monthly`: Keep one snapshot per month for N months
- `min_age_hours`: Don't delete snapshots younger than N hours
- `max_age_days`: Delete all snapshots older than N days (overrides other rules)

**Service Layer** ([`internal/hub/service/service.go:351`](internal/hub/service/service.go:351)):
- `EvaluateRetentionPolicy()`: Core retention logic with time-based grouping
- `EnqueueDeleteJob()`: Creates delete jobs targeting the correct worker
- `RunRetentionEvaluationForSource()`: Evaluates and enqueues for one source
- `RunRetentionEvaluationForAllSources()`: Batch evaluation for all sources

**Repository Layer** ([`internal/hub/repository/repository.go:562`](internal/hub/repository/repository.go:562)):
- `GetScheduleForSource()`: Get schedule with retention policy
- `ListAllSchedules()`: Get all enabled schedules
- `ListSnapshotsForRetention()`: Get snapshots ordered by created_at
- `DeleteSnapshot()`: Remove snapshot record from database

**Worker Handler** ([`internal/worker/orchestrator/orchestrator.go:295`](internal/worker/orchestrator/orchestrator.go:295)):
- `processDeleteSnapshotJob()`: Handles delete job execution
- Calls `storage.DeleteSnapshot()` to remove files from disk

**Job Completion** ([`internal/hub/service/service.go:304`](internal/hub/service/service.go:304)):
- When delete job completes successfully, snapshot record is removed from database

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/admin/retention/run` | POST | Run retention evaluation for all sources |
| `/api/v1/admin/retention/run/:id` | POST | Run retention evaluation for a specific source |
| `/api/v1/sources/:id/retention` | GET | Get retention policy for a source |
| `/api/v1/sources/:id/retention` | PUT | Update retention policy for a source |
| `/api/v1/schedules` | POST | Create schedule with retention policy |
| `/api/v1/schedules` | GET | List schedules for a tenant |
| `/api/v1/schedules/:id` | GET | Get schedule details |
| `/api/v1/schedules/:id` | PUT | Update schedule |

### Example Retention Policy

```json
{
  "keep_last_n": 7,
  "keep_daily": 30,
  "keep_weekly": 12,
  "keep_monthly": 6,
  "min_age_hours": 24,
  "max_age_days": 365
}
```

This policy:
- Keeps the 7 most recent snapshots
- Keeps one snapshot per day for 30 days
- Keeps one snapshot per week for 12 weeks
- Keeps one snapshot per month for 6 months
- Never deletes snapshots younger than 24 hours
- Deletes all snapshots older than 365 days

### Testing

```bash
# Run retention evaluation for all sources
curl -X POST http://localhost:8080/api/v1/admin/retention/run

# Run retention evaluation for a specific source
curl -X POST http://localhost:8080/api/v1/admin/retention/run/{source_id}
```

**Response Example**:
```json
{
  "results": [...],
  "summary": {
    "sources_evaluated": 1,
    "total_snapshots": 10,
    "total_to_keep": 7,
    "total_to_delete": 3,
    "jobs_enqueued": 3
  }
}
```

### Schedule Management & User-Configurable Retention

**Status**: ‚úÖ **Complete** - Users can now configure retention policies via API

Users can configure retention policies when creating or updating schedules. The retention policy is stored in the `schedules.retention_policy` JSONB column.

**API Endpoints**:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/schedules` | POST | Create schedule with retention policy |
| `/api/v1/schedules` | GET | List schedules for a tenant |
| `/api/v1/schedules/:id` | GET | Get schedule details |
| `/api/v1/schedules/:id` | PUT | Update schedule |
| `/api/v1/sources/:id/retention` | GET | Get retention policy for a source |
| `/api/v1/sources/:id/retention` | PUT | Update retention policy for a source |

**Repository Layer** ([`internal/hub/repository/repository.go:670`](internal/hub/repository/repository.go:670)):
- `CreateSchedule()`: Create a new schedule with retention policy
- `UpdateSchedule()`: Update schedule (cron, interval, status, retention)
- `UpdateScheduleRetention()`: Update only the retention policy
- `ListSchedulesByTenant()`: Get all schedules for a tenant
- `GetSchedule()`: Get schedule by ID

**Service Layer** ([`internal/hub/service/service.go:640`](internal/hub/service/service.go:640)):
- `CreateSchedule()`: Validate and create schedule
- `UpdateSchedule()`: Update schedule with validation
- `UpdateSourceRetentionPolicy()`: Quick update just the retention policy
- `GetScheduleForSource()`: Get schedule for a specific source

**Automatic Retention Scheduler** ([`cmd/hub/main.go:179`](cmd/hub/main.go:179)):
- Background goroutine runs retention evaluation periodically
- Configurable via `RETENTION_EVALUATION_INTERVAL_HOURS` (default: 6 hours)
- Runs once 30 seconds after startup, then every interval
- Logs summary of evaluation results

### Example: User Creates Backup with Retention

```bash
# 1. Create a schedule with retention policy
curl -X POST http://localhost:8080/api/v1/schedules \
  -H "Content-Type: application/json" \
  -d '{
    "tenant_id": "...",
    "source_id": "...",
    "interval_minutes": 60,
    "retention_policy": {
      "keep_last_n": 7,
      "keep_daily": 30,
      "min_age_hours": 24
    }
  }'

# 2. Update retention policy later
curl -X PUT http://localhost:8080/api/v1/sources/{source_id}/retention \
  -H "Content-Type: application/json" \
  -d '{
    "retention_policy": {
      "keep_last_n": 14,
      "keep_weekly": 8
    }
  }'
```

---

## Logging & Observability Improvements

**Status**: ‚úÖ **Complete** - Enhanced logging for better debugging and frontend integration

### Changes Implemented

| Improvement | Location | Description |
|-------------|----------|-------------|
| Worker error logging | [`internal/worker/orchestrator/orchestrator.go:141-147`](internal/worker/orchestrator/orchestrator.go:141) | Worker now logs full error message when jobs fail |
| Hub log noise reduction | [`internal/hub/handlers/handlers.go:217-224`](internal/hub/handlers/handlers.go:217) | "No jobs available" is no longer logged as error (expected when queue is empty) |
| Error propagation | Repository ‚Üí Service ‚Üí Handler | `sql.ErrNoRows` is passed through without wrapping to distinguish "no jobs" from actual errors |

### Before vs After

**Worker Logs (Failed Job):**
- Before: `worker worker-1 completed job abc123 with status: failed`
- After: `worker worker-1 completed job abc123 with status: failed, error: failed to connect: failed to dial SSH: ssh: handshake failed...`

**Hub Logs (Empty Queue):**
- Before: `failed to claim job: failed to claim job: failed to claim job: sql: no rows in result set` (every 5 seconds)
- After: *(Silent - no logging when queue is empty)*

### Database Storage

All job errors are stored in the `jobs.error_message` column for frontend API access:

```sql
SELECT id, status, error_message FROM jobs WHERE status = 'failed';
```

---

## Step 7: Restore Service & Download Management

**Status**: ‚úÖ **Complete** - Full restore pipeline with configurable download expiration

**Goal**: Enable restore downloads in v0 with admin-configurable settings

| Task | Status | Notes |
|------|--------|-------|
| 7.1 | `POST /api/v1/snapshots/:id/restore` endpoint | ‚úÖ | [`internal/hub/handlers/handlers.go:421`](internal/hub/handlers/handlers.go) |
| 7.2 | Hub enqueues restore job targeted to `snapshot.worker_id` | ‚úÖ | Restores routed to owning worker |
| 7.3 | Restore service: claim and process restore jobs | ‚úÖ | Separate `cmd/restore` service |
| 7.4 | Restore service reads encrypted backup from shared storage | ‚úÖ | Read-only mount of worker volume |
| 7.5 | Restore service decrypts and extracts to temp dir | ‚úÖ | Age decryption with tenant private key |
| 7.6 | Restore service creates ZIP archive for download | ‚úÖ | `archive/zip` for portability |
| 7.7 | Restore service reports completion with download metadata | ‚úÖ | Token, URL, expires_at saved to database |
| 7.8 | Token-based download mechanism with expiration | ‚úÖ | Configurable via system settings |

### Implementation Details

**Restore Service Architecture** ([`cmd/restore/main.go`](cmd/restore/main.go)):
- Separate service from worker (can scale independently)
- Shares worker storage via read-only Docker volume
- Fetches download expiration from Hub on startup
- Generates secure tokens for download access
- Cleanup goroutine removes expired tokens every 5 minutes

**Download Server** ([`internal/restore/download/server.go`](internal/restore/download/server.go)):
- Fiber-based HTTP server on `:8082`
- Token-based download: `GET /download/:token`
- Configurable expiration time (in hours)
- Token info includes: snapshot_id, file_path, created_at, expires_at

**API Endpoints**:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/snapshots/:id/restore` | POST | Enqueue restore job |
| `/api/v1/admin/settings` | GET | List all system settings |
| `/api/v1/admin/settings/:key` | GET | Get specific setting |
| `/api/v1/admin/settings/:key` | PUT | Update setting |
| `/internal/settings/download-expiration` | GET | Internal: get download expiration hours |
| `/internal/restore-jobs/claim` | POST | Restore service claims job |
| `/internal/restore-jobs/:id/complete` | POST | Restore service reports completion |
| `/download/:token` | GET | Download restored backup (restore service) |

### Database Changes

**Migration 00002** ([`internal/hub/database/migrations/00002_add_download_tracking.sql`](internal/hub/database/migrations/00002_add_download_tracking.sql)):
- Added `download_token`, `download_expires_at`, `download_url` columns to `snapshots`
- Created `system_settings` table for configuration
- Default settings: `download_expiration_hours=1`, `max_download_size_mb=5000`

**Repository Layer** ([`internal/hub/repository/repository.go:819`](internal/hub/repository/repository.go:819)):
- `GetSetting()`: Retrieve setting by key
- `ListSettings()`: Get all settings
- `UpsertSetting()`: Create or update setting
- `UpdateSnapshotDownloadInfo()`: Store download metadata
- `GetSnapshotByDownloadToken()`: Lookup snapshot by token

### Configurable Download Token Expiration

**Status**: ‚úÖ **Complete** - Admin-configurable via API, persisted to database

**Implementation**:

| Component | Location | Description |
|-----------|----------|-------------|
| Migration | [`00002_add_download_tracking.sql`](internal/hub/database/migrations/00002_add_download_tracking.sql) | `system_settings` table with default values |
| Repository | [`repository.go:828-907`](internal/hub/repository/repository.go#L828) | CRUD operations for settings |
| Service | [`service.go:1043-1097`](internal/hub/service/service.go#L1043) | Business logic for settings management |
| Handlers | [`handlers.go:562-623`](internal/hub/handlers/handlers.go#L562) | Admin API endpoints |
| Restore Client | [`client.go:56-68`](internal/restore/client/client.go#L56) | Fetch expiration from Hub |
| Restore Main | [`main.go:31-46`](cmd/restore/main.go#L31) | Fetch on startup, pass to server |
| Download Server | [`server.go:94-111`](internal/restore/download/server.go#L94) | Use configured expiration |

**Settings API**:
```bash
# List all settings
curl http://localhost:8080/api/v1/admin/settings

# Get specific setting
curl http://localhost:8080/api/v1/admin/settings/download_expiration_hours

# Update download expiration (e.g., to 24 hours)
curl -X PUT http://localhost:8080/api/v1/admin/settings/download_expiration_hours \
  -H "Content-Type: application/json" \
  -d '{"value":"24"}'

# Verify change (restore service logs will show new value on restart)
```

**Settings Table Schema**:
```sql
CREATE TABLE system_settings (
    key TEXT PRIMARY KEY,
    value TEXT NOT NULL,
    description TEXT,
    created_at TIMESTAMP NOT NULL DEFAULT now(),
    updated_at TIMESTAMP NOT NULL DEFAULT now()
);
```

**Default Settings**:
| Key | Value | Description |
|-----|-------|-------------|
| `download_expiration_hours` | `1` | Default download link expiration time in hours |
| `max_download_size_mb` | `5000` | Maximum size for downloads in MB |

### Testing

**End-to-End Restore Test**:
```bash
# 1. Trigger restore
curl -X POST http://localhost:8080/api/v1/snapshots/{snapshot_id}/restore

# 2. Restore service processes job, creates download token
# Check restore service logs for:
# "download expiration set to X hours"

# 3. Download the backup
curl -O http://localhost:8082/download/{token}

# 4. Verify snapshot has download metadata
curl http://localhost:8080/api/v1/snapshots/{snapshot_id}
# Returns: download_token, download_expires_at, download_url
```

**Test Results**:
| Test | Expiration Hours | Status |
|------|------------------|--------|
| Initial test | 24 | ‚úÖ Token created with 24-hour expiry |
| Reconfiguration | 6 | ‚úÖ Restarted service, tokens use 6 hours |
| Download | N/A | ‚úÖ ZIP file downloaded and extracted successfully |

---

## Deferred to v1 (S3/Garage Storage)

| Feature | Notes |
|---------|-------|
| S3/Garage upload module | Worker uploads after local write |
| Scoped credential generation | Per-tenant or per-source S3 credentials |
| Presigned URL downloads | Native restore downloads |
| Multi-worker cross-disk restores | Shared storage enables any worker to restore |
| Advanced dedupe (Kopia) | Optional optimization |

---

## Deferred to v2+ (Frontend Dashboard & Authentication)

| Feature | Framework | Notes |
|---------|-----------|-------|
| **Frontend Dashboard** | **Vue.js** | v1 = API testing only (cURL/Postman), v2 = UI development |
| **User Authentication** | JWT + Fiber middleware | v1 = no auth or simple API key, v2 = full JWT |
| **Multi-user Support** | Vue.js + Hub API | v1 = single tenant or simple auth, v2 = multi-tenant |
| **Admin Dashboard** | Vue.js | v3+ - system monitoring, user management, billing |

**Frontend Stack (v2+):**
- Framework: Vue.js 3
- API Client: Axios or native fetch
- Authentication: JWT tokens stored in httpOnly cookies or localStorage
- State Management: Pinia (if needed)
- Build Tool: Vite

---

## Additional Connectors (Post-MVP)

After SSH/SFTP is working, add these connectors incrementally:

| Connector | Status | Notes |
|-----------|--------|-------|
| FTP (files only) | üîÑ | Simpler than SSH, no remote command execution |
| MySQL dump (direct) | üîÑ | Connect directly to TCP port |
| PostgreSQL dump (direct) | üîÑ | Connect directly to TCP port |
| MySQL dump (via SSH) | üîÑ | SSH tunnel or remote mysqldump |
| PostgreSQL dump (via SSH) | üîÑ | SSH tunnel or remote pg_dump |
| WordPress (over SSH) | üîÑ | wp-config.php + files via SSH |

---

## Development Checklist

When starting a new task:

1. **Read relevant documentation:**
   - [docs/architecture.md](architecture.md) - For architecture decisions
   - [docs/data-model.md](data-model.md) - For database schema
   - [docs/dev-start.md](dev-start.md) - For development sequence
   - [docs/plan.md](plan.md) - For implementation milestones

2. **Update this file:**
   - Mark task as üöß **In Progress**
   - Add any notes or decisions made

3. **Implement:**
   - Write code following the monorepo structure
   - Keep shared types in `/pkg` only
   - Don't cross-import `/internal` between services

4. **Test:**
   - Test locally with `docker compose`
   - Update task status to ‚úÖ **Done** when passing
   - Document any issues in Notes column

5. **Move to next task**

---

## Quick Reference Commands

```bash
# Start full dev stack (with auto-migrate enabled)
docker compose --env-file deploy/.env -f deploy/docker-compose.yml up --build

# Build services locally
CGO_ENABLED=0 go build -o bin/hub ./cmd/hub
CGO_ENABLED=0 go build -o bin/worker ./cmd/worker

# Run migrations manually
export DATABASE_URL="postgres://xvault:xvault@localhost:5432/xvault?sslmode=disable"
./bin/hub -migrate

# Check migration status
./bin/hub -migrate-status

# Run Hub with auto-migrate (for local development)
export DATABASE_URL="postgres://xvault:xvault@localhost:5432/xvault?sslmode=disable"
export REDIS_URL="redis://localhost:6379/0"
export HUB_AUTO_MIGRATE="true"
./bin/hub

# Run Worker
export WORKER_ID="worker-1"
export WORKER_STORAGE_BASE="/var/lib/xvault/backups"
export HUB_BASE_URL="http://localhost:8080"
export REDIS_URL="redis://localhost:6379/0"
./bin/worker

# Run tests
go test ./...

# Check logs
docker compose logs hub
docker compose logs worker
```

---

## Key Architecture Reminders

1. **Multi-tenancy**: Always use opaque IDs (`tenant_id`, `source_id`, `snapshot_id`) - never user-provided names in paths
2. **Storage path**: `/var/lib/xvault/backups/tenants/{tenant_id}/sources/{source_id}/snapshots/{snapshot_id}/`
3. **Temp path**: `/tmp/gobackup/{job_id}/` (aggressive cleanup required)
4. **No secrets in Redis**: Job payloads reference `credential_id`, not plaintext
5. **Hub is control plane only**: Never transfers backup data
6. **Worker routing**: Restore/delete jobs must target the `worker_id` that owns the snapshot
